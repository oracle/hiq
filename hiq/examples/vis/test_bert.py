"""
ğŸ”µ
â”œâ”€â”€ embeddings(BertEmbeddings)
â”‚   â”œâ”€â”€ word_embeddings(Embedding) weight[30522, 768]
â”‚   â”œâ”€â”€ position_embeddings(Embedding) weight[512, 768]
â”‚   â”œâ”€â”€ token_type_embeddings(Embedding) weight[2, 768]
â”‚   â””â”€â”€ LayerNorm(LayerNorm) weight[768] bias[768]
â”œâ”€â”€ encoder(BertEncoder)
â”‚   â””â”€â”€ layer(ModuleList)
â”‚       â””â”€â”€ +0-11(BertLayer)
â”‚           â”œâ”€â”€ attention(BertAttention)
â”‚           â”‚   â”œâ”€â”€ self(BertSelfAttention)
â”‚           â”‚   â”‚   â””â”€â”€ +query,key,value(Linear) weight[768, 768] bias[768]
â”‚           â”‚   â””â”€â”€ output(BertSelfOutput)
â”‚           â”‚       â”œâ”€â”€ dense(Linear) weight[768, 768] bias[768]
â”‚           â”‚       â””â”€â”€ LayerNorm(LayerNorm) weight[768] bias[768]
â”‚           â”œâ”€â”€ intermediate(BertIntermediate)
â”‚           â”‚   â””â”€â”€ dense(Linear) weight[3072, 768] bias[3072]
â”‚           â””â”€â”€ output(BertOutput)
â”‚               â”œâ”€â”€ dense(Linear) weight[768, 3072] bias[768]
â”‚               â””â”€â”€ LayerNorm(LayerNorm) weight[768] bias[768]
â””â”€â”€ pooler(BertPooler)
    â””â”€â”€ dense(Linear) weight[768, 768] bias[768]
********************************************************************************
ğŸ”µ
â”œâ”€â”€ embeddings(BertEmbeddings)
â”‚   â”œâ”€â”€ word_embeddings(Embedding) weight[30522, 768](cuda:0)
â”‚   â”œâ”€â”€ position_embeddings(Embedding) weight[512, 768](cuda:0)
â”‚   â”œâ”€â”€ token_type_embeddings(Embedding) weight[2, 768](cuda:0)
â”‚   â””â”€â”€ LayerNorm(LayerNorm) weight[768](cuda:0) bias[768](cuda:0)
â”œâ”€â”€ encoder(BertEncoder)
â”‚   â””â”€â”€ layer(ModuleList)
â”‚       â”œâ”€â”€ 0(BertLayer)
â”‚       â”‚   â”œâ”€â”€ attention(BertAttention)
â”‚       â”‚   â”‚   â”œâ”€â”€ self(BertSelfAttention)
â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ query(Linear) weight[768, 768](cuda:0)â„ï¸  bias[768](cuda:0)
â”‚       â”‚   â”‚   â”‚   â””â”€â”€ +key,value(Linear) weight[768, 768](cuda:0) bias[768](cuda:0)
â”‚       â”‚   â”‚   â””â”€â”€ output(BertSelfOutput)
â”‚       â”‚   â”‚       â”œâ”€â”€ dense(Linear) weight[768, 768](cuda:0) bias[768](cuda:0)
â”‚       â”‚   â”‚       â””â”€â”€ LayerNorm(LayerNorm) weight[768](cuda:0) bias[768](cuda:0)
â”‚       â”‚   â”œâ”€â”€ intermediate(BertIntermediate)
â”‚       â”‚   â”‚   â””â”€â”€ dense(Linear) weight[3072, 768](cuda:0) bias[3072](cuda:0)
â”‚       â”‚   â””â”€â”€ output(BertOutput)
â”‚       â”‚       â”œâ”€â”€ dense(Linear) weight[768, 3072](cuda:0) bias[768](cuda:0)
â”‚       â”‚       â””â”€â”€ LayerNorm(LayerNorm) weight[768](cuda:0) bias[768](cuda:0)
â”‚       â””â”€â”€ +1-11(BertLayer)
â”‚           â”œâ”€â”€ attention(BertAttention)
â”‚           â”‚   â”œâ”€â”€ self(BertSelfAttention)
â”‚           â”‚   â”‚   â””â”€â”€ +query,key,value(Linear) weight[768, 768](cuda:0) bias[768](cuda:0)
â”‚           â”‚   â””â”€â”€ output(BertSelfOutput)
â”‚           â”‚       â”œâ”€â”€ dense(Linear) weight[768, 768](cuda:0) bias[768](cuda:0)
â”‚           â”‚       â””â”€â”€ LayerNorm(LayerNorm) weight[768](cuda:0) bias[768](cuda:0)
â”‚           â”œâ”€â”€ intermediate(BertIntermediate)
â”‚           â”‚   â””â”€â”€ dense(Linear) weight[3072, 768](cuda:0) bias[3072](cuda:0)
â”‚           â””â”€â”€ output(BertOutput)
â”‚               â”œâ”€â”€ dense(Linear) weight[768, 3072](cuda:0) bias[768](cuda:0)
â”‚               â””â”€â”€ LayerNorm(LayerNorm) weight[768](cuda:0) bias[768](cuda:0)
â””â”€â”€ pooler(BertPooler)
    â””â”€â”€ dense(Linear) weight[768, 768](cuda:0) bias[768](cuda:0)
"""

from transformers import BertModel
from hiq.vis import print_model

model = BertModel.from_pretrained("bert-base-uncased")

print_model(model)

print("*" * 80)
model.embeddings.word_embeddings.requires_grad = False
model.encoder.layer[0].attention.self.query.weight.requires_grad = False
model = model.cuda()
print_model(model)
